{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPX1fg4kFKPrH6n5v0liNCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j3mGImmSZYwq"},"outputs":[],"source":["!pip install transformers datasets torch scikit-learn==1.2.2 scipy==1.10.1  seqeval==1.2.2 pytorch-crf==0.7.2"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4r2ceBohZnM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/BERT-BiGRU"],"metadata":{"id":"cqm-kllpZpCf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torchcrf import CRF\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from seqeval.metrics import classification_report\n","from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, DataCollatorForTokenClassification"],"metadata":{"id":"RO25pfv7Zttg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tag_2_id = {'B-application': 0, 'B-cve id': 1, 'B-edition': 2, 'B-file': 3, 'B-function': 4, 'B-hardware': 5, 'B-language': 6, 'B-method': 7, 'B-os': 8, 'B-parameter': 9, 'B-programming language': 10, 'B-relevant_term': 11, 'B-update': 12, 'B-vendor': 13, 'B-version': 14, 'I-application': 15, 'I-edition': 16, 'I-hardware': 17, 'I-os': 18, 'I-relevant_term': 19, 'I-update': 20, 'I-vendor': 21, 'I-version': 22, 'O': 23}\n","id_2_tag = {0: 'B-application', 1: 'B-cve id', 2: 'B-edition', 3: 'B-file', 4: 'B-function', 5: 'B-hardware', 6: 'B-language', 7: 'B-method', 8: 'B-os', 9: 'B-parameter', 10: 'B-programming language', 11: 'B-relevant_term', 12: 'B-update', 13: 'B-vendor', 14: 'B-version', 15: 'I-application', 16: 'I-edition', 17: 'I-hardware', 18: 'I-os', 19: 'I-relevant_term', 20: 'I-update', 21: 'I-vendor', 22: 'I-version', 23: 'O'}"],"metadata":{"id":"9hb1lJkIZux1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_names = [v for k,v in id_2_tag.items()]"],"metadata":{"id":"1go7BWRdZv4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NerConfig:\n","    def __init__(self):\n","        self.bert_dir = \"thongnef/bert-finetuned-ner-cti\"\n","\n","        self.output_dir = \"./checkpoint/\"\n","        if not os.path.exists(self.output_dir):\n","            os.mkdir(self.output_dir)\n","\n","        self.bio_labels = label_names\n","        self.num_labels = len(self.bio_labels)\n","        self.label2id = tag_2_id\n","        self.id2label = id_2_tag\n","\n","        self.max_seq_len = 512\n","        self.epochs = 5\n","        self.train_batch_size = 8\n","        self.dev_batch_size = 8\n","        self.bert_learning_rate = 2e-5\n","        self.crf_learning_rate = 3e-3\n","        self.adam_epsilon = 1e-8\n","        self.weight_decay = 0.01\n","        self.warmup_proportion = 0.01\n","        self.save_step = 500"],"metadata":{"id":"-9tmRk4xZxxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ModelOutput:\n","    def __init__(self, logits, labels, loss=None):\n","        self.logits = logits\n","        self.labels = labels\n","        self.loss = loss\n","\n","class BertNer(nn.Module):\n","    def __init__(self, args):\n","        super(BertNer, self).__init__()\n","        self.bert = BertModel.from_pretrained(args.bert_dir)\n","        self.bert_config = BertConfig.from_pretrained(args.bert_dir)\n","        hidden_size = self.bert_config.hidden_size\n","        self.gru_hidden = 128  # Change the name to gru_hidden\n","        self.max_seq_len = args.max_seq_len\n","        self.bigru = nn.GRU(hidden_size, self.gru_hidden, 1, bidirectional=True, batch_first=True, dropout=0.1)  # Change to nn.GRU\n","        self.linear = nn.Linear(self.gru_hidden * 2, args.num_labels)  # Change to gru_hidden\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        seq_out = bert_output[0]  # [batchsize, max_len, 768]\n","        batch_size = seq_out.size(0)\n","        seq_out, _ = self.bigru(seq_out)  # Change to bigru\n","        seq_out = seq_out.contiguous().view(-1, self.gru_hidden * 2)\n","        seq_out = seq_out.contiguous().view(batch_size, self.max_seq_len, -1)\n","        seq_out = self.linear(seq_out)\n","\n","        loss = None\n","        if labels is not None:\n","            criterion = nn.CrossEntropyLoss()\n","            loss = criterion(seq_out.view(-1, args.num_labels), labels.view(-1))\n","\n","        model_output = ModelOutput(seq_out, labels, loss)\n","        return model_output\n"],"metadata":{"id":"nwH5utdsZ0FM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self,\n","                 output_dir=None,\n","                 model=None,\n","                 train_loader=None,\n","                 save_step=500,\n","                 dev_loader=None,\n","                 test_loader=None,\n","                 optimizer=None,\n","                 schedule=None,\n","                 epochs=1,\n","                 device=\"cpu\",\n","                 id2label=None):\n","        self.output_dir = output_dir\n","        self.model = model\n","        self.train_loader = train_loader\n","        self.dev_loader = dev_loader\n","        self.test_loader = test_loader\n","        self.epochs = epochs\n","        self.device = device\n","        self.optimizer = optimizer\n","        self.schedule = schedule\n","        self.id2label = id2label\n","        self.save_step = save_step\n","        self.total_step = len(self.train_loader) * self.epochs\n","\n","    def train(self):\n","        global_step = 1\n","        for epoch in range(1, self.epochs + 1):\n","            for step, batch_data in enumerate(self.train_loader):\n","                self.model.train()\n","                for key, value in batch_data.items():\n","                    batch_data[key] = value.to(self.device)\n","                input_ids = batch_data[\"input_ids\"]\n","                attention_mask = batch_data[\"attention_mask\"]\n","                labels = batch_data[\"labels\"]\n","                output = self.model(input_ids, attention_mask, labels)\n","                loss = output.loss\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","                self.schedule.step()\n","                print(f\"【train】{epoch}/{self.epochs} {global_step}/{self.total_step} loss:{loss.item()}\")\n","                global_step += 1\n","                if global_step % self.save_step == 0:\n","                    torch.save(self.model.state_dict(), os.path.join(self.output_dir, \"pytorch_model_ner.bin\"))\n","\n","\n","        torch.save(self.model.state_dict(), os.path.join(self.output_dir, \"pytorch_model_ner.bin\"))\n","\n","    def test(self):\n","        self.model.load_state_dict(torch.load(os.path.join(self.output_dir, \"pytorch_model_ner.bin\")))\n","        self.model.eval()\n","        preds = []\n","        trues = []\n","        for step, batch_data in enumerate(tqdm(self.test_loader)):\n","            for key, value in batch_data.items():\n","                batch_data[key] = value.to(self.device)\n","            input_ids = batch_data[\"input_ids\"]\n","            attention_mask = batch_data[\"attention_mask\"]\n","            labels = batch_data[\"labels\"]\n","            output = self.model(input_ids, attention_mask, labels)\n","            logits = output.logits\n","            attention_mask = attention_mask.detach().cpu().numpy()\n","            labels = labels.detach().cpu().numpy()\n","\n","            batch_size = input_ids.size(0)\n","            for i in range(batch_size):\n","                length = sum(attention_mask[i])\n","                logit = logits[i][1:length]\n","                logit = [self.id2label[i] for i in logit]\n","                label = labels[i][1:length]\n","                label = [self.id2label[i] for i in label]\n","                preds.append(logit)\n","                trues.append(label)\n","\n","        report = classification_report(trues, preds, digits=7)\n","        return report"],"metadata":{"id":"CZy66KmsZ0ks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_optimizer_and_scheduler(args, model, t_total):\n","    module = (\n","        model.module if hasattr(model, \"module\") else model\n","    )\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    model_param = list(module.named_parameters())\n","\n","    bert_param_optimizer = []\n","    other_param_optimizer = []\n","\n","    for name, para in model_param:\n","        space = name.split('.')\n","        # print(name)\n","        if space[0] == 'bert_module' or space[0] == \"bert\":\n","            bert_param_optimizer.append((name, para))\n","        else:\n","            other_param_optimizer.append((name, para))\n","\n","    optimizer_grouped_parameters = [\n","        # bert other module\n","        {\"params\": [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)],\n","         \"weight_decay\": args.weight_decay, 'lr': args.bert_learning_rate},\n","        {\"params\": [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)],\n","         \"weight_decay\": 0.0, 'lr': args.bert_learning_rate},\n","\n","        {\"params\": [p for n, p in other_param_optimizer if not any(nd in n for nd in no_decay)],\n","         \"weight_decay\": args.weight_decay, 'lr': args.crf_learning_rate},\n","        {\"params\": [p for n, p in other_param_optimizer if any(nd in n for nd in no_decay)],\n","         \"weight_decay\": 0.0, 'lr': args.crf_learning_rate},\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.bert_learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=int(args.warmup_proportion * t_total), num_training_steps=t_total\n","    )\n","\n","    return optimizer, scheduler"],"metadata":{"id":"LtKoUWfjZ2af"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#process data\n","def preprocess_data():\n","  args = NerConfig()\n","  raw_datasets = load_dataset(\"thongnef/dataset_dacn\")\n","  # tokenizer = BertTokenizer.from_pretrained(args.bert_dir)\n","  tokenizer = AutoTokenizer.from_pretrained(args.bert_dir)\n","\n","  converted_dict = {0:15, 1:1,2:16, 5:17,8:18, 10:19,12:20,13:21, 14:22 }\n","\n","  def align_labels_with_tokens(labels, word_ids):\n","      new_labels = []\n","      current_word = None\n","      for word_id in word_ids:\n","          if word_id != current_word:\n","              # Start of a new word!\n","              current_word = word_id\n","              label = -100 if word_id is None else labels[word_id]\n","              new_labels.append(label)\n","          elif word_id is None:\n","              # Special token\n","              new_labels.append(-100)\n","          else:\n","              # Same word as previous token\n","              label = labels[word_id]\n","              if label in converted_dict.keys():\n","                label = converted_dict[label]\n","              # if label % 2 == 1:\n","              #     label += 1\n","              new_labels.append(label)\n","      new_labels = [0 if x == -100 else x for x in new_labels]\n","      return new_labels\n","\n","  def tokenize_and_align_labels(examples):\n","      tokenized_inputs = tokenizer(\n","          examples[\"words\"], truncation=True, is_split_into_words=True, padding=\"max_length\"\n","      )\n","      all_labels = examples[\"tag\"]\n","      new_labels = []\n","      for i, labels in enumerate(all_labels):\n","          word_ids = tokenized_inputs.word_ids(i)\n","          new_labels.append(align_labels_with_tokens(labels, word_ids))\n","\n","      tokenized_inputs[\"labels\"] = new_labels\n","      return tokenized_inputs\n","\n","  tokenized_datasets = raw_datasets.map(\n","      tokenize_and_align_labels,\n","      batched=True,\n","      remove_columns=raw_datasets[\"train\"].column_names,\n","  )\n","\n","  tokenized_datasets = tokenized_datasets.remove_columns(\"token_type_ids\")\n","  data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","  return tokenized_datasets, data_collator\n"],"metadata":{"id":"ULiIZjCBZ4aw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    args = NerConfig()\n","\n","    with open(os.path.join(args.output_dir, \"ner_args.json\"), \"w\") as fp:\n","        json.dump(vars(args), fp, ensure_ascii=False, indent=2)\n","\n","    # tokenizer = BertTokenizer.from_pretrained(args.bert_dir)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    tokenized_datasets, data_collator = preprocess_data()\n","\n","    train_dataset = tokenized_datasets[\"train\"]\n","    dev_dataset = tokenized_datasets[\"test\"]\n","    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=args.train_batch_size, num_workers=2, collate_fn=data_collator)\n","    dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=args.dev_batch_size, num_workers=2, collate_fn=data_collator)\n","\n","    model = BertNer(args)\n","\n","    # for name,_ in model.named_parameters():\n","    #   print(name)\n","\n","    model.to(device)\n","    t_toal = len(train_loader) * args.epochs\n","    optimizer, schedule = build_optimizer_and_scheduler(args, model, t_toal)\n","\n","    train = Trainer(\n","        output_dir=args.output_dir,\n","        model=model,\n","        train_loader=train_loader,\n","        dev_loader=dev_loader,\n","        test_loader=dev_loader,\n","        optimizer=optimizer,\n","        schedule=schedule,\n","        epochs=args.epochs,\n","        device=device,\n","        id2label=args.id2label\n","    )\n","\n","    train.train()\n","\n","    report = train.test()\n","    print(report)\n"],"metadata":{"id":"nfF0vlQQaDfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"qH6C9fpqaOfI"},"execution_count":null,"outputs":[]}]}